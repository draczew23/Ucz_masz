{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZwgrY7EcDJD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C99wmXx9cDJY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maszyny Wektorów Nośnych 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B50zNbgkcDJT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Powtórka \n",
    "* warunki KKT\n",
    "\n",
    "Dla Lagrangianu z mnożnikami $\\alpha _{i}$ i $\\beta _{i}$:\n",
    "\n",
    "$\\qquad \n",
    "\\mathcal {L}(w,\\alpha ,\\beta ) = f(w) + \\sum _{i=1}^{k} \\alpha _{i}g_{i}(w) + \\sum _{i=1}^{k} \\beta _{i}h_{i}(w)\n",
    "$\n",
    "\n",
    "gdy spełnione warunki Karush-Kuhn-Tucker'a (KKT):\n",
    "\n",
    "$\\begin{matrix}\n",
    "(1) &\\frac{\\partial }{\\partial w_{i}} \\mathcal {L} (w^{*},\\alpha ^{*},\\beta ^{*})&=& 0 , \\quad i = 1,\\dots ,k \\\\\n",
    "(2) &\\frac{\\partial }{\\partial \\beta _{i}} \\mathcal {L} (w^{*},\\alpha ^{*},\\beta ^{*})&=& 0 , \\quad i = 1,\\dots ,l \\\\\n",
    "(3) &\\alpha _{i}^{*} g_{i}(w^{*}) &=&0, \\quad i = 1,\\dots ,k \\\\\n",
    "(4) &g_{i}(w^{*}) &\\le & 0, \\quad i =1,\\dots ,k \\\\\n",
    "(5) &\\alpha ^{*} &\\ge & 0, \\quad i=1,\\dots ,k\n",
    "\\end{matrix}$\n",
    "\n",
    "to \n",
    "\n",
    "$\\begin{matrix}\n",
    "d^{*} &=& \\max _{\\alpha ,\\beta : \\alpha _{i}\\ge 0} \\min _{w} \\mathcal {L}(w,\\alpha ,\\beta ) \\\\\n",
    "&=&\n",
    "\\min _{w} \\max _{\\alpha ,\\beta : \\alpha _{i}\\ge 0} \\mathcal {L}(w,\\alpha ,\\beta ) =p^{*}\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qvyeXPHcDJc",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## SVM w formaliźmie Lagranga\n",
    "\n",
    "Problem znalezienia klasyfikatora optymalnego pod względem marginesów można wyrazić w następujący sposób:\n",
    "\n",
    "$\\qquad$ $\\begin{matrix}\n",
    "\\min _{w,b} \\frac{1}{2}||w||^{2}&\\\\\n",
    "\\text{p.w.: } &y^{(j)}(w^{T}x^{(j)}+b) \\ge 1, \\quad j= 1, \\dots ,m\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itTZkz6_cDJg",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Można go przepisać w takiej postaci aby pasowała do formalizmu uogólnionej metody Lagrangea, którą omówiliśmy na poprzednim wykładzie:\n",
    "\n",
    "$\\begin{matrix}\n",
    "\\min _{ w,b} \\frac{1}{2}||w||^{2}&\\\\\n",
    "\\text{p.w.: }& g_{j}(w,b) = 1 - y^{(j)}(w^{T}x^{(j)}+b) \\le 0, \\quad j= 1, \\dots ,m\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "949P6rEjcDJk",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lagrangian dla problemu SVM w postaci pierwotnej wygląda tak:\n",
    "\n",
    "$\\mathcal {L}(w,b,\\alpha ) = \\frac{1}{2}||w||^{2} + \\sum _{j=1}^{m} \\alpha _{j}g_j(w,b)=$\n",
    "\n",
    "$\\qquad \\qquad = \\frac{1}{2}||w||^{2} + \\sum _{j=1}^{m} \\alpha _{j}\\left[1 - y^{(j)}(w^{T}x^{(j)}+b) \\right]=$\n",
    "\n",
    "$\\qquad \\qquad =\\frac{1}{2}||w||^{2} - \\sum _{j=1}^{m} \\alpha _{j}\\left[ y^{(j)}(w^{T}x^{(j)}+b) -1\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEziLskrcDJn",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* W Lagrangianie występują tylko mnożniki $\\alpha $, ponieważ mamy więzy tylko w postaci nierówności.\n",
    "* Każdy przykład $j$ z ciągu uczącego dodaje nam jeden wiąz $g_{j}$. \n",
    "* Do uogólnianego Lagrangianu warunki te wchodzą z wagami $\\alpha _{j}$.\n",
    "* z (5) warunku KKT $\\alpha_j >0 $\n",
    "* z (4) warunku KKT $g_j(w,b) \\le 0 $, spełnienie tego warunku gwarantuje to, że najgorsze marginesy są 1\n",
    "* z (3) warunku KKT wynika, że, tylko te $\\alpha _{j}$ są $>0$), dla których warunek jest spełniony z równością, tzn. $g_{j}(w,b) = 0$. \n",
    "* warunek (2) nas nie dotyczy, bo nie mamy więzów w postaci równości\n",
    "\n",
    "> przykłady $j$, dla których zachodzi warunek (3) to punkty położone najbliżej hiperpowierzchni decyzyjnej. \n",
    "> * To właśnie te punkty nazywane są wektorami nośnymi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLx-6sCGcDJp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Przejście do postaci dualnej\n",
    "\n",
    "* Pozwoli nam to na rozwiązywanie problemów, które nie są separowalne liniowo.\n",
    "* Najpierw uzyskajmy $\\theta _{d}(\\alpha) = \\min _{w,b} \\mathcal {L}(w,b,\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aaHaMyOjcDJs",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Aby to uczynić musimy zminimalizować $\\mathcal {L}$ po $w$ i $b$, trzymając $\\alpha $ stałe. W tym celu policzymy pochodną $\\mathcal {L}$ po $w$ i po $b$ i położymy je równe zero:\n",
    "\n",
    "$\\qquad \n",
    "\\nabla _{w} \\mathcal {L}(w,b,\\alpha ) = \\nabla _{w} \\left\\{\\frac{1}{2}||w||^{2} - \\sum _{j=1}^{m} \\alpha _{j}\\left[ y^{(j)}(w^{T}x^{(j)}+b) -1\\right] \\right\\}$\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\quad = \n",
    "w - \\sum _{j=1}^{m}\\alpha _{j}y^{(j)}x^{(j)} =0\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8cOKnmscDJu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stąd:\n",
    "\n",
    "$w^* = \\sum _{j=1}^{m}\\alpha _{j}y^{(j)}x^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQb4FyzacDJw",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dla $b$ mamy:\n",
    "\n",
    "$\\frac{\\partial }{\\partial b} \\mathcal {L}(w,b,\\alpha ) = \\sum _{j=1}^{m} \\alpha _{j}y^{(j)} =0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mg2g8RYLcDJ0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Jeśli teraz weźmiemy $w^*$  i wstawimy do Lagrangianu  $\\mathcal {L}$ to otrzymamy:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\theta _{d}= \\min _{w,b}\\mathcal {L}(w,b,\\alpha ) = \\sum _{j=1}^{m}\\alpha _{j} - \\frac{1}{2}\\sum _{i,j =1}^{m} y^{(i)}y^{(j)} \\alpha _{i}\\alpha _{j} (x^{(i)})^{T}x^{(j)} - b \\sum _{j=1}^{m}\\alpha _{j}y^{(j)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jke5FIzycDJ2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ale z równania na $b$ wynika, że ostatni człon tego wyrażenia jest równy zero, więc mamy:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\theta _{d}= \\min _{w,b}\\mathcal {L}(w,b,\\alpha ) = \\sum _{j=1}^{m}\\alpha _{j} - \\frac{1}{2}\\sum _{i,j =1}^{m} y^{(i)}y^{(j)} \\alpha _{i}\\alpha _{j} (x^{(i)})^{T}x^{(j)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdUC3dHhcDJ3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  wyrażenie $(x^{(i)})^{T}x^{(j)}$ to iloczyn skalarny wektorów $x^{(i)}$ oraz $x^{(j)}$, \n",
    "  * bedziemy je dalej zapisywać jako $\\langle x^{(i)},x^{(j)}\\rangle $.\n",
    "* Zatem nasz dualny problem optymalizacyjny można zapisać tak:\n",
    "\n",
    "\n",
    "$\\begin{matrix}\n",
    "\\max _{\\alpha } \\theta _{d} &= &\\sum _{j=1}^{m}\\alpha _{j} - \\frac{1}{2}\\sum _{i,j =1}^{m} y^{(i)}y^{(j)} \\alpha _{i}\\alpha _{j} \\langle x^{(i)}, x^{(j)}\\rangle \\\\\n",
    "\\text{pod warunkiem: }&& \\alpha _{j}\\ge 0, \\quad j=1,\\dots ,m\\\\\n",
    "&& \\sum _{j=1}^{m}\\alpha _{j}y^{(j)} = 0\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6-pTumUcDJ4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spełnione są warunki KKT (warunek 1 spełniliśmy licząc $w^*$), zatem rozwiązanie tego problemu dualnego jest też rozwiązaniem naszego problemu pierwotnego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Elr6vrhcDJ5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* Jak przejść od problemu pierwotnego do dualnego?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38GwOMmucDJ6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wyznaczenie parametrów modelu:\n",
    "\n",
    "Zakładając, że mamy algorytm znajdujący $\\alpha ^{*}$, które maksymalizują $\\theta _{d}$  możemy podstawić to $\\alpha ^{*}$ do równania ($w^* = \\sum _{j=1}^{m}\\alpha _{j}y^{(j)}x^{(j)}$) i wyznaczyć $w^{*}$:\n",
    "\n",
    "\n",
    "$w^{*} = \\sum _{j=1}^{m}\\alpha _{j}^{*}y^{(j)}x^{(j)}$\n",
    "\n",
    "a następnie obliczyć optymalne $b$ ze wzoru:\n",
    "\n",
    "$\\qquad$ $b^{*} = - \\frac{\\max _{j:y^{(j)} = -1}{w^{*}}^{T}x^{(j)} + \\min _{j:y^{(j)} = 1}{w^{*}}^{T}x^{(j)} }{2}$\n",
    "\n",
    "Mając $w$ oraz $b$ możemy wtedy bez problemu otrzymać nasze szukane marginesy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9hD5a-9cDJ7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Klasyfikacja:\n",
    "\n",
    "* Klasyfikując nowy przypadek $x$ musielibyśmy policzyć ${w^{*}}^{T}x + b^{*}$ i jeśli otrzymamy wartość ujemną to klasyfikujemy $x$ jako typ $-1$ a w przeciwnym wypadku jako 1:\n",
    "\n",
    "$\\begin{matrix}\n",
    "{w^{*}}^{T}x + b^{*} &=& \\left( \\sum _{j=1}^{m} \\alpha _{j}^{*}y^{(j)}x^{(j)}\\right)^{T} x + b^{*} \\\\\n",
    "&=&\\sum _{j=1}^{m} \\alpha _{j}^{*}y^{(j)} \\langle x^{(j)}, x \\rangle + b^{*} \n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RI7Q0BJ7cDJ8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zatem aby wykonać klasyfikację nowego przypadku $x$ musimy obliczyć jego iloczyn skalarny z wektorami nośnymi ze zbioru uczącego (tymi, dla których $\\alpha _{j}^{*} > 0$, dla pozostałych wektorów w zbiorze uczącym $\\alpha _{j}^{*}=0$), a tych jak już wcześniej wspominaliśmy jest zazwyczaj niewiele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgC6y7nicDJ9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* Co to są wektory nośne?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zj6DNlCscDJ-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funkcje jądrowe\n",
    "Samo obliczanie iloczynów skalarnych można przeprowadzić bardzo wydajnie stosując odpowiednie funkcje jądrowe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVBsl8MFcDJ_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mapowanie do przestrzeni wielowymiarowych\n",
    "* Nasze dotychczasowe algorytmy klasyfikacyjne, były ograniczone do rozwiązywania problemów separowalnych liniowo. \n",
    "* Okazuje się jednak, że często można uczynić problem separowalnym liniowo poprzez przemapowanie oryginalnych danych wejściowych do jakiejś więcej wymiarowej przestrzeni. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FlPlwor_cDKB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Przykład\n",
    "Dla przykładu rozważmy dwa zbiory punktów jednowymiarowych. Jeden zbiór skupiony jest wokół zera, a drugi rozłożony równomiernie po lewej i prawej jego stronie. Przechodząc ze zmiennych $x$ do $(x,x^{2})$ punkty stają się liniowo separowalne.\n",
    "\n",
    "Wykonajmy ilustrację tego przykładu.\n",
    "\n",
    "Definiujemy ciąg uczący:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxNZ2maUcDKC",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(-5,5,0.5)\n",
    "y = np.ones(x.shape)  \n",
    "y[x<-2] = -1\n",
    "y[x>2 ] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WifGOo69cDKJ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Problem jest 1-wymiarowy i wygląda tak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8tIJtEGcDKK",
    "outputId": "ff7a9c72-9d02-4fa7-f1fa-a303d6fb00ce",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "py.plot(x[y==1], 0*x[y ==1],'ro')\n",
    "py.plot(x[y==-1], 0*x[y ==-1],'bo')    \n",
    "py.xlabel(\"x\")\n",
    "fr = py.gca()\n",
    "fr.axes.get_yaxis().set_visible(False)\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awlw7cpdcDKY",
    "outputId": "2273e9ad-c7d9-495f-d955-ee310916bed5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x2 = x**2\n",
    "py.plot(x[y==1], x2[y ==1],'ro')\n",
    "py.plot(x[y==-1], x2[y ==-1],'bo')\n",
    "py.xlabel(\"x\")\n",
    "py.ylabel(\"x2\")\n",
    "py.plot([-6, 6],[5,5],'g')\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3d4TIwK0cDKe",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Teraz już nie ma problemu aby rozseparować dane liniowo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ry5nZ934cDKh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funkcja mapująca\n",
    "W ogólności wprowadzimy funkcję mapującą $\\phi (x)$, która przenosi punkty z oryginalnej przestrzeni wejściowej do rozszerzonej przestrzeni cech. W powyższym przykładzie byłoby to:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\phi (x) = \\left[\\begin{array}{c}\n",
    "x\\\\\n",
    "x^{2} \\end{array}\\right]\n",
    "$\n",
    "\n",
    "* Aby skorzystać z takiego mapowania wystarczy w naszych algorytmach uczących zamienić wszędzie $x$ na $\\phi (x)$.\n",
    "* Podobnie możemy postąpić z algorytmem SVM. \n",
    "* W postaci dualnej algorytm SVM jest wyrażony całkowicie przez iloczyny skalarne. \n",
    "* Możemy zastąpić wszystkie wyrażenia $\\langle x, z \\rangle $ przez $\\langle \\phi (x), \\phi (z) \\rangle $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrZrV3KOcDKi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jądra\n",
    "Dla danego mapowania $\\phi $ zdefiniujemy jądro (kernel):\n",
    "\n",
    "$\\qquad$ $\n",
    "K(x,z) = \\phi (x)^{T}\\phi (z)\n",
    "$\n",
    "* Jest to funkcja zwracająca iloczyn skalarny zmapowanych wektorów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EccQkB-cDKl",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* wszędzie gdzie w algorytmie występuje $\\langle x,z \\rangle $ wstawiamy $K(x,z)$ i otrzymujemy algorytm działający w przestrzeni, do której mapuje $\\phi $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRHEgDxscDKn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sprytne liczenie iloczynów skalarnych: \n",
    "* W wielu przypadkach, aby obliczyć $K(x,z)$ nie musimy wcale przechodzić całej drogi: \n",
    "\n",
    "$\\qquad$ $x \\rightarrow \\phi (x) \\rightarrow \\langle \\phi (x),\\phi (z) \\rangle $ \n",
    "\n",
    "(taka droga zresztą mogła by być niewykonalna, np. w przypadku mapowania do przestrzeni nieskończenie wymiarowej). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4v99DsncDKp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Przykład:\n",
    "\n",
    "\n",
    "$K(x,z) = ( x^{T}z)^{2}$\n",
    "\n",
    "Rozpisując to wyrażenie na współrzędne otrzymujemy:\n",
    "\n",
    "$\\qquad$ $\\begin{matrix}\n",
    "K(x,z) &=& \\left(\\sum _{i=1}^{m}x_{i}z_{i} \\right) \\left(\\sum _{j=1}^{m}x_{j}z_{j}\\right)\\\\\n",
    "&=& \\sum _{i=1}^{m}\\sum _{j=1}^{m} x_{i}x_{j}z_{i}z_{j}\\\\\n",
    "&=& \\sum _{i,j=1}^{m}(x_{i}x_{j})(z_{i}z_{j})\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ahqs2Gk7cDKr",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Widzimy tu, że jeśli popatrzeć na $K$ tak: $K(x,z) = \\phi (x)^{T}\\phi (z)$ to owo $K$ związane jest z mapowaniem $\\phi $, które w jawnej postaci dla $m=3$ wyglądałoby tak:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\phi (x) = \\left[\\begin{array}{c}\n",
    "x_{1}x_{1}\\\\\n",
    "x_{1}x_{2}\\\\\n",
    "x_{1}x_{3}\\\\\n",
    "x_{2}x_{1}\\\\\n",
    "x_{2}x_{2}\\\\\n",
    "x_{2}x_{3}\\\\\n",
    "x_{3}x_{1}\\\\\n",
    "x_{3}x_{2}\\\\\n",
    "x_{3}x_{3}\n",
    "\\end{array}\\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMSJ0x1VcDKt",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zauważmy, że samo obliczenie mapowania w tym przypadku jest operacją o złożoności obliczeniowej $O(m^{2})$ natomiast obliczenie jądra za pomocą równia ($K(x,z) = ( x^{T}z)^{2}$) jest operacją o złożoności obliczeniowej $O(m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw43rS2ZcDKu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Podobne własności ma jądro:\n",
    "\n",
    "$\\qquad$ $\n",
    "K(x,z) = (x^{T}z + c)^{2} = \\sum _{i,j=1}^{m} (x_{i}x_{j})(z_{i}z_{j}) + \\sum _{i=1}^{m}(\\sqrt{2c}x_{i} )(\\sqrt{2c}z_{i}) +c^{2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWYhePt4cDKw",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Jawna postać mapowania odpowiadającego temu jądru wygląda następująco (dla $m=3$):\n",
    "\n",
    "$\\qquad$ $\n",
    "\\phi (x) = \\left[\\begin{array}{c}\n",
    "x_{1}x_{1}\\\\\n",
    "x_{1}x_{2}\\\\\n",
    "x_{1}x_{3}\\\\\n",
    "x_{2}x_{1}\\\\\n",
    "x_{2}x_{2}\\\\\n",
    "x_{2}x_{3}\\\\\n",
    "x_{3}x_{1}\\\\\n",
    "x_{3}x_{2}\\\\\n",
    "x_{3}x_{3} \\\\\n",
    "\\sqrt{2c}x_{1}\\\\\n",
    "\\sqrt{2c}x_{2}\\\\\n",
    "\\sqrt{2c}x_{3}\\\\\n",
    "c\n",
    "\\end{array}\\right]\n",
    "$\n",
    "\n",
    "czyli zawiera zarówno wyrazy pierwszego rzędu ($x_{i}$) oraz drugiego rzędu ($x_{i}x_{j}$). Parametr $c$ kontroluje względny udział części liniowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64Bm80SEcDKx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W ogólności jądro postaci $K(x,z) = (x^{T}z + c)^{d}$ odpowiada mapowaniu do $\\binom{n+d}{d}$ wymiarowej przestrzeni parametrów, której wymiary są rozpięte przez wszystkie iloczyny typu $x_{i_{1}},x_{i_{2}},\\dots ,x_{i_{k}}$ aż do rzędu $d$. Dzięki sztuczce z jądrem nigdy nie musimy jawnie obliczać tych wielowymiarowych wektorów i obliczenia nadal mają złożoność $O(m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EX6qr81McDKy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* Co robi funkcja mapująca?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkPV25PBcDK0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* co robi funkcja jądrowa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lgazoELicDK1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jakie są inne dobre funkcje jądrowe?\n",
    "> Na jądro możemy patrzeć jak na funkcję, która jest jakąś miarą podobieństwa pomiędzy wektorami cech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPjI9gTNcDK2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* W szczególności gdyby nasze wektory cech były znormalizowane do jedynki to duża wartość jądra $K(x,z) = \\phi (x)^{T}\\phi (z)$ odpowiadałaby wektorom bardzo podobnym, zaś wartość jądra bliska zeru odpowiadałaby wektorom cech, które są do siebie prawie ortogonalne, tzn. mało podobne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arQID5F9cDK3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Idąc tym tropem możemy zapostulować także inne funkcje jądra, które w jakimś sensie mogłyby stanowić miarę podobieństwa między wektorami. \n",
    "* Popularną funkcją jest np. funkcja Gaussa, prowadząca do <b>jądra Gaussowskiego</b> następującej postaci:\n",
    "\n",
    "$\\qquad$ $\n",
    "K(x,z) = \\exp \\left( - \\frac{||x-z||^{2}}{2 \\sigma ^{2}}\\right)\n",
    "$\n",
    "\n",
    "![]()\n",
    "<img src=\"https://image.slidesharecdn.com/svm-140807035301-phpapp01/95/support-vector-machine-without-tears-28-638.jpg?cb=1407384107\" width=1600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mt8EYlLdcDK4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Jak w ogólności sprawdzić czy wymyślona przez nas funkcja jest dobrym kandydatem na jądro?\n",
    "\n",
    "* Rozważymy to najpierw na przykładzie a potem podamy ogólne twierdzenie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sULqKBkcDK5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Przykład\n",
    "Załóżmy, że mamy pewną funkcję $K$, która jest jądrem pewnego mapowania $\\phi $. Załóżmy dalej, że mamy pewien zbiór $m$ punktów $\\lbrace x^{(1)},\\dots ,x^{(m)}\\rbrace $. Zdefiniujmy macierz $\\mathbf {K}$ zwaną macierzą jądra w taki sposób, że jej $i,j$ element dany jest wzorem:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\mathbf {K}_{i,j} = K(x^{(i)},x^{(j)})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIq5k7PbcDK6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Zauważmy, że macierz ta musi być symetryczna, bo:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\mathbf {K}_{i,j} = K(x^{(i)},x^{(j)}) =\\phi (x^{(i)})^{T}\\phi (x^{(j)}) = \\phi (x^{(j)})^{T}\\phi (x^{(i)}) = K(x^{(j)},x^{(i)}) = \\mathbf {K}_{j,i}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAlTn6VLcDK6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Druga obserwacja jest następująca:\n",
    "  * Niech $\\phi _{k}(x)$ oznacza $k$-tą współrzędną wektora $\\phi (x)$. \n",
    "  * Wtedy dla dowolnego wektora $z$ mamy:\n",
    "\n",
    "$\\qquad$ $\\begin{matrix}\n",
    "z^{T}\\mathbf {K}z &=& \\sum _{i}\\sum _{j} z_{i}\\mathbf {K}_{i,j}z_{j} \\\\\n",
    "&=& \\sum _{i}\\sum _{j} z_{i}\\phi (x^{(i)})^{T}\\phi (x^{(j)})z_{j} \\\\\n",
    "&=& \\sum _{i}\\sum _{j} z_{i} \\sum _{k} \\phi _{k}(x^{(i)})\\phi _{k}(x^{(j)})z_{j} \\\\\n",
    "&=& \\sum _{k}\\sum _{i}\\sum _{j} z_{i}\\phi _{k}(x^{(i)})\\phi _{k}(x^{(j)})z_{j} \\\\\n",
    "&=& \\sum _{k} \\left( \\sum _{i} z_{i} \\phi _{k}(x^{(i)})\\right)^{2} \\\\\n",
    "&\\ge & 0\n",
    "\\end{matrix}$\n",
    "\n",
    "Ponieważ powyższe obliczenie pokazuje, że dla dowolnego $z$ wyrażenie $z^{T}\\mathbf {K}z$ jest nieujemne to oznacza, że macierz $\\mathbf {K}$ jest dodatnio określona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Xte9CrVcDK7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Twierdzenie Mercera\n",
    "* Pokazaliśmy w tym przykładzie, że:\n",
    " > jeśli mamy jakieś mapowanie $\\phi $ i związane z nim jądro $K$ to macierz jądra jest symetryczna i dodatnio określona. \n",
    "\n",
    "* Okazuje się, że jest to warunek konieczny i wystarczający, aby funkcja $K$ była jądrem, jest to twierdzenie Mercera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObcuHo_NcDK8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Zastosowania podejścia jądrowego\n",
    "Warto sobie uświadomić, że podejście \"jądrowe\" ma znacznie szersze zastosowanie niż tylko algorytm SVM. \n",
    "\n",
    "* Jeśli tylko jesteśmy w stanie wyrazić algorytm w postaci bazującej na iloczynach skalarnych $\\langle x,z \\rangle $ (da się to w szczególności zrobić np. dla regresji logistycznej) to zamiana tych iloczynów na funkcje jądra daje nam algorytm działający efektywnie w przestrzeni, do której przenosi nas odwzorowanie $\\phi $. \n",
    "* Dzięki temu można spowodować, że wiele problemów, które nie są separowalne liniowo w pierwotnej przestrzeni wejść staje się separowalna liniowo w tej nowej, więcej wymiarowej przestrzeni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEPOsQBBcDK8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* Co to jest liniowa separowalność?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "psR9jOw_cDK9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularyzcja i przypadki nieseparowalne liniowo\n",
    "\n",
    "* Zaprezentowana dotychczas wersja SVM zakładała, że dane są liniowo separowalne. \n",
    "* Sztuczka z jądrem mapującym zwiększa co prawda szansę na otrzymanie problemu liniowo separowalnego, ale nie daje na to gwarancji. \n",
    "* Co więcej w dotychczasowej wersji nasz algorytm SVM jest bardzo podatny na outliery, czyli przypadki odstające.\n",
    "(Pokażemy to na ćwiczeniach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7bDF0lacDK9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jak to poprawić?\n",
    "Aby poprawić oba te problemy można zastosować regularyzację:\n",
    "\n",
    "$\\qquad$ $\\begin{matrix}\n",
    "\\min _{ w, b}&& \\frac{1}{2}||w||^{2} + C\\sum _{j=1}^{m}\\xi _{j}\\\\\n",
    "\\text{pod warunkiem: }&& y^{(j)}(w^{T}x^{(j)} +b ) \\ge 1- \\xi _{j}, \\quad j=1,\\dots ,m\\\\\n",
    "&& \\xi _{j} \\ge 0, \\quad j=1,\\dots ,m\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndkcZZVicDK-",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Oznacza ona tyle, że zgadzamy się na to, że nie wszystkie marginesy funkcyjne są większe niż 1 (przypomnijmy, że ujemny margines funkcyjny odpowiadał złej klasyfikacji), \n",
    "* ale karzemy algorytm za naruszanie tego warunku przez zwiększanie funkcji celu.\n",
    "* Parametr $C$ kontroluje jak bardzo nie podoba nam się błędne klasyfikowanie przypadków."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_ICDGKDcDK_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lagrangian\n",
    "Formułujemy Lagrangian następującej postaci:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\mathcal {L}(w,b,\\xi ,\\alpha ,r) = \\frac{1}{2}w^{T}w + C\\sum _{j=1}^{m}\\xi _{j} - \\sum _{j=1}^{m}\\alpha _{j}[y^{(j)}(x^{T}w +b)-1 +\\xi _{j}] - \\sum _{j=1}^{m}r_{j}\\xi _{j}\n",
    "$\n",
    "\n",
    "gdzie $\\alpha _{j}\\ge 0$ i $r_{j}\\ge 0$ są mnożnikami Lagrangea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VXjRnYhcDLA",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Przejście do postaci dualnej\n",
    "Przejście do postaci dualnej polega na:\n",
    "* policzeniu pochodnej Lagrangianu względem $w$ i $b$, \n",
    "* przyrównaniu od zera i podstawieniu otrzymanych wyrażeń ponownie do Lagragianu  \n",
    "* otrzymujemy problem dualny następującej postaci:\n",
    "\n",
    "\n",
    "$\\begin{matrix}\n",
    "\\max _{\\alpha } && \\theta _{d}(\\alpha ) = \\sum _{j=1}^{m}\\alpha _{j} - \\frac{1}{2} \\sum _{i,j =1}^{m} y^{(i)}y^{(j)}\\alpha _{i}\\alpha _{j} \\langle x^{(i)},x^{(j)} \\rangle \\\\\n",
    "\\text{pod warunkiem: } && 0 \\le \\alpha _{j} \\le C, \\quad j=1,\\dots ,m\\\\\n",
    "&&\\sum _{j=1}^{m}\\alpha _{j}y^{(j)} = 0\n",
    "\\end{matrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79yX7VwFcDLB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Do rozwiązania powyższego problemu dobrze stosuje się algorytm SMO (Sequential Minimal Optimization) [[https://www.researchgate.net/publication/2624239_Sequential_Minimal_Optimization_A_Fast_Algorithm_for_Training_Support_Vector_Machines opis algorytmu zaproponowanego przez J. Platta (1998)]]. Po wyznaczeniu za jego pomocą parametrów $\\alpha $ i $b$ można wykonywać predykcję nowych przykładów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8Zga4RvcDLC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorytm SMO - sekwencyjnej minimalnej optymalizacji\n",
    "\n",
    "Zanim przejdziemy do omówienia właściwego algorytmu SMO zrobimy dygresję na temat optymalizacji osiowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-uR9FaMcDLD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optymalizacja osiowa\n",
    "\n",
    "Załóżmy, że chcemy rozwiązać następujący problem optymalizacyjny bez więzów:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\max _{\\alpha } W (\\alpha _{1},\\dots ,\\alpha _{m})\n",
    "$\n",
    "\n",
    "Jeśli funkcja $W$ jest wypukła to algorytm, który w pętli kolejno optymalizuje jedno $\\alpha _{i}$, trzymając w danym kroku optymalizacyjnym pozostałe alfy stałe, jest zbieżny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "BGdCAz1ccDLE",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a title=\"Nicoguaro [CC BY 4.0 (https://creativecommons.org/licenses/by/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Coordinate_descent.svg\"><img width=\"512\" alt=\"Coordinate descent\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Coordinate_descent.svg/512px-Coordinate_descent.svg.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4qNagW7cDLF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorytm SMO\n",
    "\n",
    "* Chcemy rozwiązać problem optymalizacyjny SVM z regularyzacją. \n",
    "* Nie da się do niego zastosować algorytmu optymalizacji osiowej bo drugi warunek narzuca więzy na $\\alpha $. \n",
    "  * Jeśli ustalimy $m-1$ wartości $\\alpha _{j}$ to ostatnia $m$-ta wartość też już jest ustalona:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\alpha _{i}y^{(i)} = -\\sum _{j \\ne i} \\alpha _{j}y^{(j)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrI6Ai1qcDLG",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lub korzystając z faktu, że $y^{(i)} = \\lbrace -1,1\\rbrace $ i mnożąc stronami przez $y^{(i)}$ mamy: $\\alpha _{i}= -y^{(i)} \\sum _{j \\ne i} \\alpha _{j}y^{(j)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "190hzYrjcDLH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zatem najmniejszy możliwy problem optymalizacyjny wymaga jednoczesnej optymalizacji dwóch parametrów $\\alpha $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohUVB5PGcDLH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Najogólniej algorytm SMO wygląda więc następująco:\n",
    "\n",
    "Powtarzaj, aż zbiegniesz:\n",
    "\n",
    "\n",
    "\n",
    "* Wybierz parę $\\alpha _{i}$ i $\\alpha _{j}$ do optymalizacji (na podstawie heurystyki szacującej, która para da największe zbliżenie do maksimum).\n",
    "\n",
    "*\tPopraw $\\theta _{d}(\\alpha )$ biorąc pod uwagę $\\alpha _{i}$ i $\\alpha _{j}$ trzymając pozostałe alfy stałe.\n",
    "\n",
    "Testem na zbieżność są tu warunki KKT, które powinny zostać spełnione z zadaną tolerancją."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUlFQQWdcDLK",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Rozważmy krok 2. powyższego algorytmu. Załóżmy, że chcemy wykonać maksymalizację ze względu na parametry $\\alpha _{1}$ i $\\alpha _{2}$ trzymając pozostałe parametry $\\alpha _{3}, \\dots ,\\alpha _{m}$ stałe. Z drugiego warunku mamy:\n",
    "\n",
    "<equation id=\"uid17\">\n",
    "$\\alpha _{1}y^{(1)}+ \\alpha _{2}y^{(2)} = - \\sum _{i=3}^{m} \\alpha _{i} y^{(i)} = \\zeta $\n",
    "</equation>\n",
    "\n",
    "gdzie $\\zeta $ jest stałą.\n",
    "Oznacza to, że punkt $(\\alpha _{1},\\alpha _{2})$ będący rozwiązaniem musi leżeć na prostej $\\alpha _{1}y^{(1)}+ \\alpha _{2}y^{(2)} =\\zeta $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xygPzjrNcDLL",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(rysunek na tablicy: prosta przecinająca kwadrat [0,C]x[0,C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_de0jk5ecDLM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Przekształcając powyższe równanie mamy:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\alpha _{1} = \\frac{ \\zeta -\\alpha _{2}y^{(2)} }{ y^{(1)}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDAcFvMbcDLN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zatem funkcja celu może być zapisana jako:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\theta _{d}(\\alpha _{1},\\alpha _{2},\\alpha _{3},\\dots ,\\alpha _{m}) = \\theta _{d}(\\frac{\\zeta - \\alpha _{2}y^{(2)}}{y^{(1)}}, \\alpha _{2},\\alpha _{3},\\dots ,\\alpha _{m})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ulS8_0pcDLO",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ponieważ trzymamy w tym kroku parametry $\\alpha _{3}, \\dots ,\\alpha _{m}$ jako stałe to funkcja celu jest funkcją kwadratową parametru $\\alpha _{2}$. \n",
    "\n",
    "* Można by ją zapisać w postaci \n",
    "$\\qquad$ $\\theta _{d}(\\alpha _{2})=a\\alpha _{2}^{2}+b\\alpha _{2}+c$\n",
    "dla odpowiednio dobranych $a,b$ i $c$. \n",
    "\n",
    "* Łatwo można zmaksymalizować analitycznie funkcję $\\theta _{d}(\\alpha _{2})$ w przypadku swobodnym, a następnie przyciąć rozwiązanie do \"pudełka\" wynikającego z więzów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "6Qt42SDWcDLP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* dlaczego rozwiązująć problem SVM rozwiazujemy w każdej iteracji względem dwóch $\\alpha$ naraz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E83Pr_bXcDLP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Wykład6_SVM2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spyder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "livereveal": {
   "progress": true,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "serif",
   "transition": "fade",
   "width": 1600
  },
  "vscode": {
   "interpreter": {
    "hash": "c288bdb25ca9e4767fa3e338ce28882ca341a961656781e13252079ed73b75e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

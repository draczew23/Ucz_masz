{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iZwgrY7EcDJD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C99wmXx9cDJY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Maszyny Wektorów Nośnych 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B50zNbgkcDJT",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Powtórka \n",
    "* warunki KKT\n",
    "\n",
    "Dla Lagrangianu z mnożnikami $\\alpha _{i}$ i $\\beta _{i}$:\n",
    "\n",
    "$\\qquad \n",
    "\\mathcal {L}(w,\\alpha ,\\beta ) = f(w) + \\sum _{i=1}^{k} \\alpha _{i}g_{i}(w) + \\sum _{i=1}^{k} \\beta _{i}h_{i}(w)\n",
    "$\n",
    "\n",
    "gdy spełnione warunki Karush-Kuhn-Tucker'a (KKT):\n",
    "\n",
    "$\\begin{matrix}\n",
    "(1) &\\frac{\\partial }{\\partial w_{i}} \\mathcal {L} (w^{*},\\alpha ^{*},\\beta ^{*})&=& 0 , \\quad i = 1,\\dots ,k \\\\\n",
    "(2) &\\frac{\\partial }{\\partial \\beta _{i}} \\mathcal {L} (w^{*},\\alpha ^{*},\\beta ^{*})&=& 0 , \\quad i = 1,\\dots ,l \\\\\n",
    "(3) &\\alpha _{i}^{*} g_{i}(w^{*}) &=&0, \\quad i = 1,\\dots ,k \\\\\n",
    "(4) &g_{i}(w^{*}) &\\le & 0, \\quad i =1,\\dots ,k \\\\\n",
    "(5) &\\alpha ^{*} &\\ge & 0, \\quad i=1,\\dots ,k\n",
    "\\end{matrix}$\n",
    "\n",
    "to \n",
    "\n",
    "$\\begin{matrix}\n",
    "d^{*} &=& \\max _{\\alpha ,\\beta : \\alpha _{i}\\ge 0} \\min _{w} \\mathcal {L}(w,\\alpha ,\\beta ) \\\\\n",
    "&=&\n",
    "\\min _{w} \\max _{\\alpha ,\\beta : \\alpha _{i}\\ge 0} \\mathcal {L}(w,\\alpha ,\\beta ) =p^{*}\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5qvyeXPHcDJc",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## SVM w formaliźmie Lagranga\n",
    "\n",
    "Problem znalezienia klasyfikatora optymalnego pod względem marginesów można wyrazić w następujący sposób:\n",
    "\n",
    "$\\qquad$ $\\begin{matrix}\n",
    "\\min _{w,b} \\frac{1}{2}||w||^{2}&\\\\\n",
    "\\text{p.w.: } &y^{(j)}(w^{T}x^{(j)}+b) \\ge 1, \\quad j= 1, \\dots ,m\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itTZkz6_cDJg",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Można go przepisać w takiej postaci aby pasowała do formalizmu uogólnionej metody Lagrangea, którą omówiliśmy na poprzednim wykładzie:\n",
    "\n",
    "$\\begin{matrix}\n",
    "\\min _{ w,b} \\frac{1}{2}||w||^{2}&\\\\\n",
    "\\text{p.w.: }& g_{j}(w,b) = 1 - y^{(j)}(w^{T}x^{(j)}+b) \\le 0, \\quad j= 1, \\dots ,m\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "949P6rEjcDJk",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lagrangian dla problemu SVM w postaci pierwotnej wygląda tak:\n",
    "\n",
    "$\\mathcal {L}(w,b,\\alpha ) = \\frac{1}{2}||w||^{2} + \\sum _{j=1}^{m} \\alpha _{j}g_j(w,b)=$\n",
    "\n",
    "$\\qquad \\qquad = \\frac{1}{2}||w||^{2} + \\sum _{j=1}^{m} \\alpha _{j}\\left[1 - y^{(j)}(w^{T}x^{(j)}+b) \\right]=$\n",
    "\n",
    "$\\qquad \\qquad =\\frac{1}{2}||w||^{2} - \\sum _{j=1}^{m} \\alpha _{j}\\left[ y^{(j)}(w^{T}x^{(j)}+b) -1\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEziLskrcDJn",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* W Lagrangianie występują tylko mnożniki $\\alpha $, ponieważ mamy więzy tylko w postaci nierówności.\n",
    "* Każdy przykład $j$ z ciągu uczącego dodaje nam jeden wiąz $g_{j}$. \n",
    "* Do uogólnianego Lagrangianu warunki te wchodzą z wagami $\\alpha _{j}$.\n",
    "* z (5) warunku KKT $\\alpha_j >0 $\n",
    "* z (4) warunku KKT $g_j(w,b) \\le 0 $, spełnienie tego warunku gwarantuje to, że najgorsze marginesy są 1\n",
    "* z (3) warunku KKT wynika, że, tylko te $\\alpha _{j}$ są $>0$), dla których warunek jest spełniony z równością, tzn. $g_{j}(w,b) = 0$. \n",
    "* warunek (2) nas nie dotyczy, bo nie mamy więzów w postaci równości\n",
    "\n",
    "> przykłady $j$, dla których zachodzi warunek (3) to punkty położone najbliżej hiperpowierzchni decyzyjnej. \n",
    "> * To właśnie te punkty nazywane są wektorami nośnymi. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fLx-6sCGcDJp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Przejście do postaci dualnej\n",
    "\n",
    "* Pozwoli nam to na rozwiązywanie problemów, które nie są separowalne liniowo.\n",
    "* Najpierw uzyskajmy $\\theta _{d}(\\alpha) = \\min _{w,b} \\mathcal {L}(w,b,\\alpha)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aaHaMyOjcDJs",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Aby to uczynić musimy zminimalizować $\\mathcal {L}$ po $w$ i $b$, trzymając $\\alpha $ stałe. W tym celu policzymy pochodną $\\mathcal {L}$ po $w$ i po $b$ i położymy je równe zero:\n",
    "\n",
    "$\\qquad \n",
    "\\nabla _{w} \\mathcal {L}(w,b,\\alpha ) = \\nabla _{w} \\left\\{\\frac{1}{2}||w||^{2} - \\sum _{j=1}^{m} \\alpha _{j}\\left[ y^{(j)}(w^{T}x^{(j)}+b) -1\\right] \\right\\}$\n",
    "\n",
    "$\\qquad \\qquad \\qquad \\quad = \n",
    "w - \\sum _{j=1}^{m}\\alpha _{j}y^{(j)}x^{(j)} =0\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C8cOKnmscDJu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Stąd:\n",
    "\n",
    "$w^* = \\sum _{j=1}^{m}\\alpha _{j}y^{(j)}x^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DQb4FyzacDJw",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Dla $b$ mamy:\n",
    "\n",
    "$\\frac{\\partial }{\\partial b} \\mathcal {L}(w,b,\\alpha ) = \\sum _{j=1}^{m} \\alpha _{j}y^{(j)} =0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mg2g8RYLcDJ0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Jeśli teraz weźmiemy $w^*$  i wstawimy do Lagrangianu  $\\mathcal {L}$ to otrzymamy:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\theta _{d}= \\min _{w,b}\\mathcal {L}(w,b,\\alpha ) = \\sum _{j=1}^{m}\\alpha _{j} - \\frac{1}{2}\\sum _{i,j =1}^{m} y^{(i)}y^{(j)} \\alpha _{i}\\alpha _{j} (x^{(i)})^{T}x^{(j)} - b \\sum _{j=1}^{m}\\alpha _{j}y^{(j)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jke5FIzycDJ2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ale z równania na $b$ wynika, że ostatni człon tego wyrażenia jest równy zero, więc mamy:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\theta _{d}= \\min _{w,b}\\mathcal {L}(w,b,\\alpha ) = \\sum _{j=1}^{m}\\alpha _{j} - \\frac{1}{2}\\sum _{i,j =1}^{m} y^{(i)}y^{(j)} \\alpha _{i}\\alpha _{j} (x^{(i)})^{T}x^{(j)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HdUC3dHhcDJ3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  wyrażenie $(x^{(i)})^{T}x^{(j)}$ to iloczyn skalarny wektorów $x^{(i)}$ oraz $x^{(j)}$, \n",
    "  * bedziemy je dalej zapisywać jako $\\langle x^{(i)},x^{(j)}\\rangle $.\n",
    "* Zatem nasz dualny problem optymalizacyjny można zapisać tak:\n",
    "\n",
    "\n",
    "$\\begin{matrix}\n",
    "\\max _{\\alpha } \\theta _{d} &= &\\sum _{j=1}^{m}\\alpha _{j} - \\frac{1}{2}\\sum _{i,j =1}^{m} y^{(i)}y^{(j)} \\alpha _{i}\\alpha _{j} \\langle x^{(i)}, x^{(j)}\\rangle \\\\\n",
    "\\text{pod warunkiem: }&& \\alpha _{j}\\ge 0, \\quad j=1,\\dots ,m\\\\\n",
    "&& \\sum _{j=1}^{m}\\alpha _{j}y^{(j)} = 0\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f6-pTumUcDJ4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Spełnione są warunki KKT (warunek 1 spełniliśmy licząc $w^*$), zatem rozwiązanie tego problemu dualnego jest też rozwiązaniem naszego problemu pierwotnego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Elr6vrhcDJ5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* Jak przejść od problemu pierwotnego do dualnego?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "38GwOMmucDJ6",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Wyznaczenie parametrów modelu:\n",
    "\n",
    "Zakładając, że mamy algorytm znajdujący $\\alpha ^{*}$, które maksymalizują $\\theta _{d}$  możemy podstawić to $\\alpha ^{*}$ do równania ($w^* = \\sum _{j=1}^{m}\\alpha _{j}y^{(j)}x^{(j)}$) i wyznaczyć $w^{*}$:\n",
    "\n",
    "\n",
    "$w^{*} = \\sum _{j=1}^{m}\\alpha _{j}^{*}y^{(j)}x^{(j)}$\n",
    "\n",
    "a następnie obliczyć optymalne $b$ ze wzoru:\n",
    "\n",
    "$\\qquad$ $b^{*} = - \\frac{\\max _{j:y^{(j)} = -1}{w^{*}}^{T}x^{(j)} + \\min _{j:y^{(j)} = 1}{w^{*}}^{T}x^{(j)} }{2}$\n",
    "\n",
    "Mając $w$ oraz $b$ możemy wtedy bez problemu otrzymać nasze szukane marginesy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9hD5a-9cDJ7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Klasyfikacja:\n",
    "\n",
    "* Klasyfikując nowy przypadek $x$ musielibyśmy policzyć ${w^{*}}^{T}x + b^{*}$ i jeśli otrzymamy wartość ujemną to klasyfikujemy $x$ jako typ $-1$ a w przeciwnym wypadku jako 1:\n",
    "\n",
    "$\\begin{matrix}\n",
    "{w^{*}}^{T}x + b^{*} &=& \\left( \\sum _{j=1}^{m} \\alpha _{j}^{*}y^{(j)}x^{(j)}\\right)^{T} x + b^{*} \\\\\n",
    "&=&\\sum _{j=1}^{m} \\alpha _{j}^{*}y^{(j)} \\langle x^{(j)}, x \\rangle + b^{*} \n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RI7Q0BJ7cDJ8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zatem aby wykonać klasyfikację nowego przypadku $x$ musimy obliczyć jego iloczyn skalarny z wektorami nośnymi ze zbioru uczącego (tymi, dla których $\\alpha _{j}^{*} > 0$, dla pozostałych wektorów w zbiorze uczącym $\\alpha _{j}^{*}=0$), a tych jak już wcześniej wspominaliśmy jest zazwyczaj niewiele."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PgC6y7nicDJ9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* Co to są wektory nośne?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zj6DNlCscDJ-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funkcje jądrowe\n",
    "Samo obliczanie iloczynów skalarnych można przeprowadzić bardzo wydajnie stosując odpowiednie funkcje jądrowe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVBsl8MFcDJ_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mapowanie do przestrzeni wielowymiarowych\n",
    "* Nasze dotychczasowe algorytmy klasyfikacyjne, były ograniczone do rozwiązywania problemów separowalnych liniowo. \n",
    "* Okazuje się jednak, że często można uczynić problem separowalnym liniowo poprzez przemapowanie oryginalnych danych wejściowych do jakiejś więcej wymiarowej przestrzeni. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FlPlwor_cDKB",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Przykład\n",
    "Dla przykładu rozważmy dwa zbiory punktów jednowymiarowych. Jeden zbiór skupiony jest wokół zera, a drugi rozłożony równomiernie po lewej i prawej jego stronie. Przechodząc ze zmiennych $x$ do $(x,x^{2})$ punkty stają się liniowo separowalne.\n",
    "\n",
    "Wykonajmy ilustrację tego przykładu.\n",
    "\n",
    "Definiujemy ciąg uczący:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pxNZ2maUcDKC",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(-5,5,0.5)\n",
    "y = np.ones(x.shape)  \n",
    "y[x<-2] = -1\n",
    "y[x>2 ] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WifGOo69cDKJ",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Problem jest 1-wymiarowy i wygląda tak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m8tIJtEGcDKK",
    "outputId": "ff7a9c72-9d02-4fa7-f1fa-a303d6fb00ce",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1061ab9e8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "py.plot(x[y==1], 0*x[y ==1],'ro')\n",
    "py.plot(x[y==-1], 0*x[y ==-1],'bo')    \n",
    "py.xlabel(\"x\")\n",
    "fr = py.gca()\n",
    "fr.axes.get_yaxis().set_visible(False)\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awlw7cpdcDKY",
    "outputId": "2273e9ad-c7d9-495f-d955-ee310916bed5",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1062a4dd8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x2 = x**2\n",
    "py.plot(x[y==1], x2[y ==1],'ro')\n",
    "py.plot(x[y==-1], x2[y ==-1],'bo')\n",
    "py.xlabel(\"x\")\n",
    "py.ylabel(\"x2\")\n",
    "py.plot([-6, 6],[5,5],'g')\n",
    "py.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3d4TIwK0cDKe",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Teraz już nie ma problemu aby rozseparować dane liniowo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ry5nZ934cDKh",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Funkcja mapująca\n",
    "W ogólności wprowadzimy funkcję mapującą $\\phi (x)$, która przenosi punkty z oryginalnej przestrzeni wejściowej do rozszerzonej przestrzeni cech. W powyższym przykładzie byłoby to:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\phi (x) = \\left[\\begin{array}{c}\n",
    "x\\\\\n",
    "x^{2} \\end{array}\\right]\n",
    "$\n",
    "\n",
    "* Aby skorzystać z takiego mapowania wystarczy w naszych algorytmach uczących zamienić wszędzie $x$ na $\\phi (x)$.\n",
    "* Podobnie możemy postąpić z algorytmem SVM. \n",
    "* W postaci dualnej algorytm SVM jest wyrażony całkowicie przez iloczyny skalarne. \n",
    "* Możemy zastąpić wszystkie wyrażenia $\\langle x, z \\rangle $ przez $\\langle \\phi (x), \\phi (z) \\rangle $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrZrV3KOcDKi",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jądra\n",
    "Dla danego mapowania $\\phi $ zdefiniujemy jądro (kernel):\n",
    "\n",
    "$\\qquad$ $\n",
    "K(x,z) = \\phi (x)^{T}\\phi (z)\n",
    "$\n",
    "* Jest to funkcja zwracająca iloczyn skalarny zmapowanych wektorów. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EccQkB-cDKl",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* wszędzie gdzie w algorytmie występuje $\\langle x,z \\rangle $ wstawiamy $K(x,z)$ i otrzymujemy algorytm działający w przestrzeni, do której mapuje $\\phi $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iRHEgDxscDKn",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sprytne liczenie iloczynów skalarnych: \n",
    "* W wielu przypadkach, aby obliczyć $K(x,z)$ nie musimy wcale przechodzić całej drogi: \n",
    "\n",
    "$\\qquad$ $x \\rightarrow \\phi (x) \\rightarrow \\langle \\phi (x),\\phi (z) \\rangle $ \n",
    "\n",
    "(taka droga zresztą mogła by być niewykonalna, np. w przypadku mapowania do przestrzeni nieskończenie wymiarowej). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4v99DsncDKp",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Przykład:\n",
    "\n",
    "\n",
    "$K(x,z) = ( x^{T}z)^{2}$\n",
    "\n",
    "Rozpisując to wyrażenie na współrzędne otrzymujemy:\n",
    "\n",
    "$\\qquad$ $\\begin{matrix}\n",
    "K(x,z) &=& \\left(\\sum _{i=1}^{m}x_{i}z_{i} \\right) \\left(\\sum _{j=1}^{m}x_{j}z_{j}\\right)\\\\\n",
    "&=& \\sum _{i=1}^{m}\\sum _{j=1}^{m} x_{i}x_{j}z_{i}z_{j}\\\\\n",
    "&=& \\sum _{i,j=1}^{m}(x_{i}x_{j})(z_{i}z_{j})\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ahqs2Gk7cDKr",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Widzimy tu, że jeśli popatrzeć na $K$ tak: $K(x,z) = \\phi (x)^{T}\\phi (z)$ to owo $K$ związane jest z mapowaniem $\\phi $, które w jawnej postaci dla $m=3$ wyglądałoby tak:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\phi (x) = \\left[\\begin{array}{c}\n",
    "x_{1}x_{1}\\\\\n",
    "x_{1}x_{2}\\\\\n",
    "x_{1}x_{3}\\\\\n",
    "x_{2}x_{1}\\\\\n",
    "x_{2}x_{2}\\\\\n",
    "x_{2}x_{3}\\\\\n",
    "x_{3}x_{1}\\\\\n",
    "x_{3}x_{2}\\\\\n",
    "x_{3}x_{3}\n",
    "\\end{array}\\right]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMSJ0x1VcDKt",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zauważmy, że samo obliczenie mapowania w tym przypadku jest operacją o złożoności obliczeniowej $O(m^{2})$ natomiast obliczenie jądra za pomocą równia ($K(x,z) = ( x^{T}z)^{2}$) jest operacją o złożoności obliczeniowej $O(m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bw43rS2ZcDKu",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Podobne własności ma jądro:\n",
    "\n",
    "$\\qquad$ $\n",
    "K(x,z) = (x^{T}z + c)^{2} = \\sum _{i,j=1}^{m} (x_{i}x_{j})(z_{i}z_{j}) + \\sum _{i=1}^{m}(\\sqrt{2c}x_{i} )(\\sqrt{2c}z_{i}) +c^{2}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWYhePt4cDKw",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Jawna postać mapowania odpowiadającego temu jądru wygląda następująco (dla $m=3$):\n",
    "\n",
    "$\\qquad$ $\n",
    "\\phi (x) = \\left[\\begin{array}{c}\n",
    "x_{1}x_{1}\\\\\n",
    "x_{1}x_{2}\\\\\n",
    "x_{1}x_{3}\\\\\n",
    "x_{2}x_{1}\\\\\n",
    "x_{2}x_{2}\\\\\n",
    "x_{2}x_{3}\\\\\n",
    "x_{3}x_{1}\\\\\n",
    "x_{3}x_{2}\\\\\n",
    "x_{3}x_{3} \\\\\n",
    "\\sqrt{2c}x_{1}\\\\\n",
    "\\sqrt{2c}x_{2}\\\\\n",
    "\\sqrt{2c}x_{3}\\\\\n",
    "c\n",
    "\\end{array}\\right]\n",
    "$\n",
    "\n",
    "czyli zawiera zarówno wyrazy pierwszego rzędu ($x_{i}$) oraz drugiego rzędu ($x_{i}x_{j}$). Parametr $c$ kontroluje względny udział części liniowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64Bm80SEcDKx",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "W ogólności jądro postaci $K(x,z) = (x^{T}z + c)^{d}$ odpowiada mapowaniu do $\\binom{n+d}{d}$ wymiarowej przestrzeni parametrów, której wymiary są rozpięte przez wszystkie iloczyny typu $x_{i_{1}},x_{i_{2}},\\dots ,x_{i_{k}}$ aż do rzędu $d$. Dzięki sztuczce z jądrem nigdy nie musimy jawnie obliczać tych wielowymiarowych wektorów i obliczenia nadal mają złożoność $O(m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EX6qr81McDKy",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* Co robi funkcja mapująca?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PkPV25PBcDK0",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* co robi funkcja jądrowa?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lgazoELicDK1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jakie są inne dobre funkcje jądrowe?\n",
    "> Na jądro możemy patrzeć jak na funkcję, która jest jakąś miarą podobieństwa pomiędzy wektorami cech."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mPjI9gTNcDK2",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* W szczególności gdyby nasze wektory cech były znormalizowane do jedynki to duża wartość jądra $K(x,z) = \\phi (x)^{T}\\phi (z)$ odpowiadałaby wektorom bardzo podobnym, zaś wartość jądra bliska zeru odpowiadałaby wektorom cech, które są do siebie prawie ortogonalne, tzn. mało podobne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "arQID5F9cDK3",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Idąc tym tropem możemy zapostulować także inne funkcje jądra, które w jakimś sensie mogłyby stanowić miarę podobieństwa między wektorami. \n",
    "* Popularną funkcją jest np. funkcja Gaussa, prowadząca do <b>jądra Gaussowskiego</b> następującej postaci:\n",
    "\n",
    "$\\qquad$ $\n",
    "K(x,z) = \\exp \\left( - \\frac{||x-z||^{2}}{2 \\sigma ^{2}}\\right)\n",
    "$\n",
    "\n",
    "![]()\n",
    "<img src=\"https://image.slidesharecdn.com/svm-140807035301-phpapp01/95/support-vector-machine-without-tears-28-638.jpg?cb=1407384107\" width=1600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mt8EYlLdcDK4",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Jak w ogólności sprawdzić czy wymyślona przez nas funkcja jest dobrym kandydatem na jądro?\n",
    "\n",
    "* Rozważymy to najpierw na przykładzie a potem podamy ogólne twierdzenie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2sULqKBkcDK5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Przykład\n",
    "Załóżmy, że mamy pewną funkcję $K$, która jest jądrem pewnego mapowania $\\phi $. Załóżmy dalej, że mamy pewien zbiór $m$ punktów $\\lbrace x^{(1)},\\dots ,x^{(m)}\\rbrace $. Zdefiniujmy macierz $\\mathbf {K}$ zwaną macierzą jądra w taki sposób, że jej $i,j$ element dany jest wzorem:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\mathbf {K}_{i,j} = K(x^{(i)},x^{(j)})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIq5k7PbcDK6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Zauważmy, że macierz ta musi być symetryczna, bo:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\mathbf {K}_{i,j} = K(x^{(i)},x^{(j)}) =\\phi (x^{(i)})^{T}\\phi (x^{(j)}) = \\phi (x^{(j)})^{T}\\phi (x^{(i)}) = K(x^{(j)},x^{(i)}) = \\mathbf {K}_{j,i}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GAlTn6VLcDK6",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Druga obserwacja jest następująca:\n",
    "  * Niech $\\phi _{k}(x)$ oznacza $k$-tą współrzędną wektora $\\phi (x)$. \n",
    "  * Wtedy dla dowolnego wektora $z$ mamy:\n",
    "\n",
    "$\\qquad$ $\\begin{matrix}\n",
    "z^{T}\\mathbf {K}z &=& \\sum _{i}\\sum _{j} z_{i}\\mathbf {K}_{i,j}z_{j} \\\\\n",
    "&=& \\sum _{i}\\sum _{j} z_{i}\\phi (x^{(i)})^{T}\\phi (x^{(j)})z_{j} \\\\\n",
    "&=& \\sum _{i}\\sum _{j} z_{i} \\sum _{k} \\phi _{k}(x^{(i)})\\phi _{k}(x^{(j)})z_{j} \\\\\n",
    "&=& \\sum _{k}\\sum _{i}\\sum _{j} z_{i}\\phi _{k}(x^{(i)})\\phi _{k}(x^{(j)})z_{j} \\\\\n",
    "&=& \\sum _{k} \\left( \\sum _{i} z_{i} \\phi _{k}(x^{(i)})\\right)^{2} \\\\\n",
    "&\\ge & 0\n",
    "\\end{matrix}$\n",
    "\n",
    "Ponieważ powyższe obliczenie pokazuje, że dla dowolnego $z$ wyrażenie $z^{T}\\mathbf {K}z$ jest nieujemne to oznacza, że macierz $\\mathbf {K}$ jest dodatnio określona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Xte9CrVcDK7",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Twierdzenie Mercera\n",
    "* Pokazaliśmy w tym przykładzie, że:\n",
    " > jeśli mamy jakieś mapowanie $\\phi $ i związane z nim jądro $K$ to macierz jądra jest symetryczna i dodatnio określona. \n",
    "\n",
    "* Okazuje się, że jest to warunek konieczny i wystarczający, aby funkcja $K$ była jądrem, jest to twierdzenie Mercera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObcuHo_NcDK8",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Zastosowania podejścia jądrowego\n",
    "Warto sobie uświadomić, że podejście \"jądrowe\" ma znacznie szersze zastosowanie niż tylko algorytm SVM. \n",
    "\n",
    "* Jeśli tylko jesteśmy w stanie wyrazić algorytm w postaci bazującej na iloczynach skalarnych $\\langle x,z \\rangle $ (da się to w szczególności zrobić np. dla regresji logistycznej) to zamiana tych iloczynów na funkcje jądra daje nam algorytm działający efektywnie w przestrzeni, do której przenosi nas odwzorowanie $\\phi $. \n",
    "* Dzięki temu można spowodować, że wiele problemów, które nie są separowalne liniowo w pierwotnej przestrzeni wejść staje się separowalna liniowo w tej nowej, więcej wymiarowej przestrzeni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEPOsQBBcDK8",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* Co to jest liniowa separowalność?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "psR9jOw_cDK9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularyzcja i przypadki nieseparowalne liniowo\n",
    "\n",
    "* Zaprezentowana dotychczas wersja SVM zakładała, że dane są liniowo separowalne. \n",
    "* Sztuczka z jądrem mapującym zwiększa co prawda szansę na otrzymanie problemu liniowo separowalnego, ale nie daje na to gwarancji. \n",
    "* Co więcej w dotychczasowej wersji nasz algorytm SVM jest bardzo podatny na outliery, czyli przypadki odstające.\n",
    "(Pokażemy to na ćwiczeniach)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7bDF0lacDK9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Jak to poprawić?\n",
    "Aby poprawić oba te problemy można zastosować regularyzację:\n",
    "\n",
    "$\\qquad$ $\\begin{matrix}\n",
    "\\min _{ w, b}&& \\frac{1}{2}||w||^{2} + C\\sum _{j=1}^{m}\\xi _{j}\\\\\n",
    "\\text{pod warunkiem: }&& y^{(j)}(w^{T}x^{(j)} +b ) \\ge 1- \\xi _{j}, \\quad j=1,\\dots ,m\\\\\n",
    "&& \\xi _{j} \\ge 0, \\quad j=1,\\dots ,m\n",
    "\\end{matrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndkcZZVicDK-",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Oznacza ona tyle, że zgadzamy się na to, że nie wszystkie marginesy funkcyjne są większe niż 1 (przypomnijmy, że ujemny margines funkcyjny odpowiadał złej klasyfikacji), \n",
    "* ale karzemy algorytm za naruszanie tego warunku przez zwiększanie funkcji celu.\n",
    "* Parametr $C$ kontroluje jak bardzo nie podoba nam się błędne klasyfikowanie przypadków."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_ICDGKDcDK_",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lagrangian\n",
    "Formułujemy Lagrangian następującej postaci:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\mathcal {L}(w,b,\\xi ,\\alpha ,r) = \\frac{1}{2}w^{T}w + C\\sum _{j=1}^{m}\\xi _{j} - \\sum _{j=1}^{m}\\alpha _{j}[y^{(j)}(x^{T}w +b)-1 +\\xi _{j}] - \\sum _{j=1}^{m}r_{j}\\xi _{j}\n",
    "$\n",
    "\n",
    "gdzie $\\alpha _{j}\\ge 0$ i $r_{j}\\ge 0$ są mnożnikami Lagrangea. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VXjRnYhcDLA",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Przejście do postaci dualnej\n",
    "Przejście do postaci dualnej polega na:\n",
    "* policzeniu pochodnej Lagrangianu względem $w$ i $b$, \n",
    "* przyrównaniu od zera i podstawieniu otrzymanych wyrażeń ponownie do Lagragianu  \n",
    "* otrzymujemy problem dualny następującej postaci:\n",
    "\n",
    "\n",
    "$\\begin{matrix}\n",
    "\\max _{\\alpha } && \\theta _{d}(\\alpha ) = \\sum _{j=1}^{m}\\alpha _{j} - \\frac{1}{2} \\sum _{i,j =1}^{m} y^{(i)}y^{(j)}\\alpha _{i}\\alpha _{j} \\langle x^{(i)},x^{(j)} \\rangle \\\\\n",
    "\\text{pod warunkiem: } && 0 \\le \\alpha _{j} \\le C, \\quad j=1,\\dots ,m\\\\\n",
    "&&\\sum _{j=1}^{m}\\alpha _{j}y^{(j)} = 0\n",
    "\\end{matrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "79yX7VwFcDLB",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Do rozwiązania powyższego problemu dobrze stosuje się algorytm SMO (Sequential Minimal Optimization) [[https://www.researchgate.net/publication/2624239_Sequential_Minimal_Optimization_A_Fast_Algorithm_for_Training_Support_Vector_Machines opis algorytmu zaproponowanego przez J. Platta (1998)]]. Po wyznaczeniu za jego pomocą parametrów $\\alpha $ i $b$ można wykonywać predykcję nowych przykładów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A8Zga4RvcDLC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorytm SMO - sekwencyjnej minimalnej optymalizacji\n",
    "\n",
    "Zanim przejdziemy do omówienia właściwego algorytmu SMO zrobimy dygresję na temat optymalizacji osiowej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6-uR9FaMcDLD",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optymalizacja osiowa\n",
    "\n",
    "Załóżmy, że chcemy rozwiązać następujący problem optymalizacyjny bez więzów:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\max _{\\alpha } W (\\alpha _{1},\\dots ,\\alpha _{m})\n",
    "$\n",
    "\n",
    "Jeśli funkcja $W$ jest wypukła to algorytm, który w pętli kolejno optymalizuje jedno $\\alpha _{i}$, trzymając w danym kroku optymalizacyjnym pozostałe alfy stałe, jest zbieżny."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "BGdCAz1ccDLE",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<a title=\"Nicoguaro [CC BY 4.0 (https://creativecommons.org/licenses/by/4.0)], from Wikimedia Commons\" href=\"https://commons.wikimedia.org/wiki/File:Coordinate_descent.svg\"><img width=\"512\" alt=\"Coordinate descent\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e3/Coordinate_descent.svg/512px-Coordinate_descent.svg.png\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4qNagW7cDLF",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algorytm SMO\n",
    "\n",
    "* Chcemy rozwiązać problem optymalizacyjny SVM z regularyzacją. \n",
    "* Nie da się do niego zastosować algorytmu optymalizacji osiowej bo drugi warunek narzuca więzy na $\\alpha $. \n",
    "  * Jeśli ustalimy $m-1$ wartości $\\alpha _{j}$ to ostatnia $m$-ta wartość też już jest ustalona:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\alpha _{i}y^{(i)} = -\\sum _{j \\ne i} \\alpha _{j}y^{(j)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrI6Ai1qcDLG",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lub korzystając z faktu, że $y^{(i)} = \\lbrace -1,1\\rbrace $ i mnożąc stronami przez $y^{(i)}$ mamy: $\\alpha _{i}= -y^{(i)} \\sum _{j \\ne i} \\alpha _{j}y^{(j)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "190hzYrjcDLH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zatem najmniejszy możliwy problem optymalizacyjny wymaga jednoczesnej optymalizacji dwóch parametrów $\\alpha $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohUVB5PGcDLH",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Najogólniej algorytm SMO wygląda więc następująco:\n",
    "\n",
    "Powtarzaj, aż zbiegniesz:\n",
    "\n",
    "\n",
    "\n",
    "* Wybierz parę $\\alpha _{i}$ i $\\alpha _{j}$ do optymalizacji (na podstawie heurystyki szacującej, która para da największe zbliżenie do maksimum).\n",
    "\n",
    "*\tPopraw $\\theta _{d}(\\alpha )$ biorąc pod uwagę $\\alpha _{i}$ i $\\alpha _{j}$ trzymając pozostałe alfy stałe.\n",
    "\n",
    "Testem na zbieżność są tu warunki KKT, które powinny zostać spełnione z zadaną tolerancją."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUlFQQWdcDLK",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Rozważmy krok 2. powyższego algorytmu. Załóżmy, że chcemy wykonać maksymalizację ze względu na parametry $\\alpha _{1}$ i $\\alpha _{2}$ trzymając pozostałe parametry $\\alpha _{3}, \\dots ,\\alpha _{m}$ stałe. Z drugiego warunku mamy:\n",
    "\n",
    "<equation id=\"uid17\">\n",
    "$\n",
    "\\alpha _{1}y^{(1)}+ \\alpha _{2}y^{(2)} = - \\sum _{i=3}^{m} \\alpha _{i} y^{(i)} = \\zeta \n",
    "$\n",
    "</equation>\n",
    "\n",
    "gdzie $\\zeta $ jest stałą.\n",
    "Oznacza to, że punkt $(\\alpha _{1},\\alpha _{2})$ będący rozwiązaniem musi leżeć na prostej $\\alpha _{1}y^{(1)}+ \\alpha _{2}y^{(2)} =\\zeta $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xygPzjrNcDLL",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "(rysunek na tablicy: prosta przecinająca kwadrat [0,C]x[0,C])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_de0jk5ecDLM",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Przekształcając powyższe równanie mamy:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\alpha _{1} = \\frac{ \\zeta -\\alpha _{2}y^{(2)} }{ y^{(1)}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDAcFvMbcDLN",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zatem funkcja celu może być zapisana jako:\n",
    "\n",
    "$\\qquad$ $\n",
    "\\theta _{d}(\\alpha _{1},\\alpha _{2},\\alpha _{3},\\dots ,\\alpha _{m}) = \\theta _{d}(\\frac{\\zeta - \\alpha _{2}y^{(2)}}{y^{(1)}}, \\alpha _{2},\\alpha _{3},\\dots ,\\alpha _{m})\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ulS8_0pcDLO",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Ponieważ trzymamy w tym kroku parametry $\\alpha _{3}, \\dots ,\\alpha _{m}$ jako stałe to funkcja celu jest funkcją kwadratową parametru $\\alpha _{2}$. \n",
    "\n",
    "* Można by ją zapisać w postaci \n",
    "$\\qquad$ $\\theta _{d}(\\alpha _{2})=a\\alpha _{2}^{2}+b\\alpha _{2}+c$\n",
    "dla odpowiednio dobranych $a,b$ i $c$. \n",
    "\n",
    "* Łatwo można zmaksymalizować analitycznie funkcję $\\theta _{d}(\\alpha _{2})$ w przypadku swobodnym, a następnie przyciąć rozwiązanie do \"pudełka\" wynikającego z więzów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "6Qt42SDWcDLP",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quiz\n",
    "* dlaczego rozwiązująć problem SVM rozwiazujemy w każdej iteracji względem dwóch $\\alpha$ naraz?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E83Pr_bXcDLP"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Wykład6_SVM2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('spyder')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "livereveal": {
   "progress": true,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "serif",
   "transition": "fade",
   "width": 1600
  },
  "vscode": {
   "interpreter": {
    "hash": "c288bdb25ca9e4767fa3e338ce28882ca341a961656781e13252079ed73b75e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
